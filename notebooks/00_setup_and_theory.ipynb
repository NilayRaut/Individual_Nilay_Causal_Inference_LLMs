{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Level Causal Inference in Large Language Models: From Prompt Engineering to Attention Mechanisms\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook provides a comprehensive guide to applying causal inference methods to Large Language Models (LLMs), bridging the gap between traditional causal analysis and modern deep learning. Readers will learn how to identify and estimate causal effects in LLMs through two complementary approaches: external causality (prompt-level interventions using propensity score matching) and internal causality (attention mechanisms as mediators using mediation analysis). The role of causal inference in ML is critical for understanding *why* models behave in specific ways, moving beyond correlation to establish causal claims about prompt effectiveness, feature importance, and model behavior. Practical applications include optimizing prompt engineering strategies, interpreting attention patterns in reasoning, and improving model interpretability through rigorous causal methods. This work demonstrates how causal frameworks can systematically analyze LLM behavior, providing methodological tools for researchers and practitioners seeking to understand and improve language model performance.\n",
    "\n",
    "**Key Learning Outcomes:**\n",
    "- Foundational causal inference concepts with LLM applications\n",
    "- Propensity score matching for prompt-level causal analysis\n",
    "- Causal mediation analysis for understanding attention mechanisms\n",
    "- Data preparation techniques for causal inference in ML contexts\n",
    "- Practical implementation using DoWhy, CausalML, and modern LLM frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Theory of Causal Inference in Machine Learning\n",
    "\n",
    "## 1.1 Causality Fundamentals\n",
    "\n",
    "### Correlation vs Causation in LLMs\n",
    "\n",
    "In machine learning, we frequently observe correlations: \"Models trained on more data tend to perform better\" or \"Longer prompts yield better completions.\" However, correlation does not imply causation. For example, longer prompts might correlate with better completions simply because longer prompts provide more context, not because length itself causes improvement. This distinction is critical when designing experiments and interpreting model behavior.\n",
    "\n",
    "### The Potential Outcomes Framework (Rubin Causal Model)\n",
    "\n",
    "The **potential outcomes framework**, developed by Donald Rubin, provides a formal language for causal inference. For each unit (e.g., a prompt-task pair), we define:\n",
    "\n",
    "- **Y_i(1)**: Potential outcome if unit receives treatment\n",
    "- **Y_i(0)**: Potential outcome if unit does not receive treatment\n",
    "- **T_i**: Treatment indicator (1 if treated, 0 if control)\n",
    "\n",
    "The fundamental problem of causal inference is that we can only observe one outcome per unit (the realized outcome), never both potential outcomes simultaneously. This motivates the need for causal methods that can estimate counterfactuals.\n",
    "\n",
    "**Individual Treatment Effect (ITE):** \n",
    "$$\\tau_i = Y_i(1) - Y_i(0)$$\n",
    "\n",
    "**Average Treatment Effect (ATE):**\n",
    "$$\\tau = \\mathbb{E}[Y(1) - Y(0)] = \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)]$$\n",
    "\n",
    "In LLM contexts, examples include:\n",
    "- Treatment: Using chain-of-thought prompting vs. direct prompting\n",
    "- Outcome: Task performance (accuracy, completion quality)\n",
    "- Causal question: Does chain-of-thought *cause* improved performance, or correlate with task types where it's naturally more effective?\n",
    "\n",
    "### Causal Graphs (Directed Acyclic Graphs - DAGs)\n",
    "\n",
    "Causal graphs, formalized by Judea Pearl, represent causal relationships visually and mathematically:\n",
    "\n",
    "- **Nodes**: Variables (treatment, outcome, confounders)\n",
    "- **Edges**: Direct causal relationships\n",
    "- **Paths**: Sequences of edges connecting variables\n",
    "\n",
    "Key path types:\n",
    "1. **Causal path**: T → O (treatment causes outcome)\n",
    "2. **Backdoor path**: T ← X → O (confounding through X)\n",
    "3. **Front-door path**: T → M → O (mediation through M)\n",
    "\n",
    "### Key Assumptions\n",
    "\n",
    "1. **Unconfoundedness**: Given observed covariates X, treatment assignment is independent of potential outcomes:\n",
    "   $$\\{Y(1), Y(0)\\} \\perp\\perp T \\mid X$$\n",
    "   \n",
    "2. **Stable Unit Treatment Value Assumption (SUTVA)**: One unit's treatment doesn't affect another's outcome, and there's no hidden variation in treatment effects.\n",
    "\n",
    "3. **Positivity (Overlap)**: Every unit has a non-zero probability of receiving each treatment level:\n",
    "   $$0 < P(T=1 \\mid X) < 1 \\quad \\forall X$$\n",
    "\n",
    "These assumptions are crucial for causal identification and will be explicitly checked in our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for theory section\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    'matplotlib', 'seaborn', 'networkx', 'numpy', 'pandas',\n",
    "    'scikit-learn', 'dowhy', 'causalml'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG 1: Confounding in LLM Experiments\n",
    "\n",
    "This DAG illustrates a common confounding scenario: the relationship between instruction format (treatment) and task completion quality (outcome) is confounded by task difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DAG for Example 1: Confounding in Instruction Format\n",
    "G1 = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "G1.add_node('Task_Difficulty', pos=(0, 2), color='lightblue', style='filled')\n",
    "G1.add_node('Prompt_Length', pos=(2, 2), color='lightblue', style='filled')\n",
    "G1.add_node('Instruction_Format', pos=(1, 0), color='lightgreen', style='filled')\n",
    "G1.add_node('Task_Completion', pos=(1, -2), color='salmon', style='filled')\n",
    "\n",
    "# Add edges\n",
    "G1.add_edge('Task_Difficulty', 'Instruction_Format')\n",
    "G1.add_edge('Task_Difficulty', 'Task_Completion')\n",
    "G1.add_edge('Prompt_Length', 'Instruction_Format')\n",
    "G1.add_edge('Prompt_Length', 'Task_Completion')\n",
    "G1.add_edge('Instruction_Format', 'Task_Completion')\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "pos = nx.get_node_attributes(G1, 'pos')\n",
    "colors = [G1.nodes[n]['color'] for n in G1.nodes()]\n",
    "\n",
    "nx.draw(G1, pos, ax=ax, with_labels=True, node_color=colors, \n",
    "        node_size=4000, font_size=10, font_weight='bold',\n",
    "        arrowsize=20, edge_color='gray', width=2)\n",
    "\n",
    "# Add annotations\n",
    "ax.text(0.5, 1.5, 'Confounders', ha='center', fontsize=12, fontweight='bold', color='blue')\n",
    "ax.text(1, -0.5, 'Treatment', ha='center', fontsize=12, fontweight='bold', color='green')\n",
    "ax.text(1, -2.7, 'Outcome', ha='center', fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "plt.title('Figure 1: Confounding in LLM Instruction Format Experiment', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\\nKey Insights from DAG:\n",
    "-------------------------\n",
    "• Task Difficulty affects BOTH instruction format choice and completion quality\n",
    "  → Creates spurious correlation between format and completion\n",
    "• Prompt Length is another confounder\n",
    "• Backdoor path: Format ← Difficulty → Completion (must be blocked)\n",
    "• Causal path: Format → Completion (true effect we want to estimate)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Propensity Score Matching\n",
    "\n",
    "### Why Matching?\n",
    "\n",
    "Propensity Score Matching (PSM) creates balanced treatment and control groups by matching units with similar probabilities of receiving treatment, given their covariates. This addresses confounding by ensuring that treated and control units are comparable on observed characteristics.\n",
    "\n",
    "**Intuition**: If we match tasks that have equal probability of being assigned Format B vs Format A, any difference in outcomes can be attributed to the format itself, not to underlying differences in task difficulty or other confounders.\n",
    "\n",
    "### Propensity Score Definition\n",
    "\n",
    "The propensity score is the probability of treatment assignment conditional on observed covariates:\n",
    "\n",
    "$$e(X) = P(T=1 \\mid X)$$\n",
    "\n",
    "where X represents observed confounders (task difficulty, prompt length, etc.).\n",
    "\n",
    "**Key Property (Rosenbaum & Rubin, 1983):**\n",
    "If treatment assignment is strongly ignorable given X, then it is also strongly ignorable given the propensity score e(X):\n",
    "\n",
    "$$\\{Y(1), Y(0)\\} \\perp\\perp T \\mid e(X)$$\n",
    "\n",
    "This allows us to reduce multidimensional covariates to a single scalar while preserving balancing properties.\n",
    "\n",
    "### Matching Algorithms\n",
    "\n",
    "1. **Nearest Neighbor Matching**: Match each treated unit to the control unit with the closest propensity score\n",
    "2. **Caliper Matching**: Only match if scores are within a specified tolerance (e.g., 0.1 standard deviations)\n",
    "3. **Kernel Matching**: Use weighted averages of all controls, with weights inversely proportional to distance\n",
    "\n",
    "### Balance Checking\n",
    "\n",
    "Before and after matching, we must check balance:\n",
    "\n",
    "- **Standardized Mean Difference (SMD)**: $\\frac{\\bar{X}_T - \\bar{X}_C}{\\sqrt{(s_T^2 + s_C^2)/2}}$\n",
    "  - SMD < 0.1 indicates good balance\n",
    "- **Variance Ratio**: Close to 1 indicates similar variances\n",
    "- **Visual inspection**: Density plots of covariates by treatment status\n",
    "\n",
    "### Treatment Effect Estimation\n",
    "\n",
    "After matching, we estimate:\n",
    "\n",
    "- **ATE (Average Treatment Effect)**: $\\frac{1}{N} \\sum_{i=1}^N (2T_i - 1) \\cdot (Y_i - Y_{m(i)})$\n",
    "- **ATT (Average Treatment Effect on the Treated)**: More common in practice\n",
    "\n",
    "**Confidence Intervals**: Use bootstrapping or analytical methods for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG 2: Mediation in Attention Mechanisms\n",
    "\n",
    "This DAG illustrates causal mediation: the effect of prompt intervention on reasoning quality is mediated through attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DAG for Example 2: Mediation in Attention Mechanisms\n",
    "G2 = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "G2.add_node('Task_Complexity', pos=(0, 2), color='lightblue', style='filled')\n",
    "G2.add_node('Prompt_Intervention', pos=(1, 0), color='lightgreen', style='filled')\n",
    "G2.add_node('Attention_Patterns', pos=(2, 0), color='lightyellow', style='filled')\n",
    "G2.add_node('Reasoning_Quality', pos=(1, -2), color='salmon', style='filled')\n",
    "\n",
    "# Add edges\n",
    "G2.add_edge('Task_Complexity', 'Prompt_Intervention')\n",
    "G2.add_edge('Task_Complexity', 'Attention_Patterns')\n",
    "G2.add_edge('Prompt_Intervention', 'Attention_Patterns')\n",
    "G2.add_edge('Prompt_Intervention', 'Reasoning_Quality')\n",
    "G2.add_edge('Attention_Patterns', 'Reasoning_Quality')\n",
    "\n",
    "# Draw\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "pos = nx.get_node_attributes(G2, 'pos')\n",
    "colors = [G2.nodes[n]['color'] for n in G2.nodes()]\n",
    "\n",
    "nx.draw(G2, pos, ax=ax, with_labels=True, node_color=colors,\n",
    "        node_size=4500, font_size=9, font_weight='bold',\n",
    "        arrowsize=20, edge_color='gray', width=2)\n",
    "\n",
    "# Add path annotations\n",
    "ax.arrow(1, -0.2, 0.9, 0, head_width=0.1, head_length=0.1, \n",
    "         fc='red', ec='red', alpha=0.5, linestyle='--', linewidth=2)\n",
    "ax.arrow(2, -0.2, -0.9, -1.5, head_width=0.1, head_length=0.1,\n",
    "         fc='red', ec='red', alpha=0.5, linestyle='--', linewidth=2)\n",
    "\n",
    "ax.text(1.5, -0.5, 'Mediator', ha='center', fontsize=11, fontweight='bold', color='orange')\n",
    "ax.text(1.5, -1.3, 'Indirect Effect', ha='center', fontsize=9, color='red', rotation=-30)\n",
    "\n",
    "plt.title('Figure 2: Causal Mediation in LLM Attention Mechanisms', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\\nKey Insights from DAG:\n",
    "-------------------------\n",
    "• Prompt Intervention affects Reasoning Quality through TWO paths:\n",
    "  1. Direct Effect: Prompt → Reasoning (unmediated)\n",
    "  2. Indirect Effect: Prompt → Attention → Reasoning (mediated)\n",
    "• Task Complexity confounds the relationship\n",
    "• Attention Patterns act as a mediator (explanation mechanism)\n",
    "• Total Effect = Direct Effect + Indirect Effect\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Causal Mediation Analysis\n",
    "\n",
    "### Direct vs Indirect Effects\n",
    "\n",
    "Causal mediation analysis decomposes the total treatment effect into:\n",
    "\n",
    "- **Direct Effect (DE)**: Effect of treatment on outcome *not* through the mediator\n",
    "- **Indirect Effect (IE)**: Effect of treatment on outcome *through* the mediator\n",
    "\n",
    "Mathematically:\n",
    "$$\\tau_{\\text{total}} = \\tau_{\\text{direct}} + \\tau_{\\text{indirect}}$$\n",
    "\n",
    "### Mediator Definition and Role\n",
    "\n",
    "A **mediator** is a variable that lies on the causal path between treatment and outcome:\n",
    "\n",
    "Treatment (T) → Mediator (M) → Outcome (Y)\n",
    "\n",
    "Key requirements:\n",
    "1. M is causally affected by T\n",
    "2. M causally affects Y\n",
    "3. M is not a collider\n",
    "\n",
    "### Mediation in LLM Context\n",
    "\n",
    "In our Example 2:\n",
    "- **Treatment**: Chain-of-thought prompting vs. direct prompting\n",
    "- **Mediator**: Attention patterns in specific heads/layers\n",
    "- **Outcome**: Reasoning quality (accuracy, logical coherence)\n",
    "\n",
    "**Question**: Does CoT improve reasoning *because* it changes how the model attends to information (indirect effect), or does it have other mechanisms (direct effect)?\n",
    "\n",
    "### Path Decomposition\n",
    "\n",
    "Using the counterfactual framework (Imai et al., 2010):\n",
    "\n",
    "**Total Effect**:\n",
    "$$TE = \\mathbb{E}[Y(1, M(1)) - Y(0, M(0))]$$\n",
    "\n",
    "**Direct Effect** (natural):\n",
    "$$NDE = \\mathbb{E}[Y(1, M(0)) - Y(0, M(0))]$$\n",
    "\n",
    "**Indirect Effect** (natural):\n",
    "$$NIE = \\mathbb{E}[Y(1, M(1)) - Y(1, M(0))]$$\n",
    "\n",
    "where $M(t)$ is the counterfactual mediator value under treatment $t$.\n",
    "\n",
    "### Sequential Ignorability Assumption\n",
    "\n",
    "For mediation analysis, we require:\n",
    "\n",
    "1. **No unmeasured confounding of T → Y**: $\\{Y(1,m), Y(0,m)\\} \\perp\\perp T \\mid X$\n",
    "2. **No unmeasured confounding of M → Y**: $\\{Y(1,m), Y(0,m)\\} \\perp\\perp M \\mid T, X$\n",
    "\n",
    "This is a strong assumption and motivates sensitivity analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Preparation for Causal Inference\n",
    "\n",
    "### Feature Selection: Confounders vs Colliders\n",
    "\n",
    "In causal analysis, feature selection is not just about predictive power:\n",
    "\n",
    "- **Confounders**: Variables that affect BOTH treatment and outcome. **Must be included** to avoid bias.\n",
    "- **Mediators**: Variables on the causal path. **Exclude** when estimating total effect, **include** when doing mediation analysis.\n",
    "- **Colliders**: Variables affected by BOTH treatment and outcome. **Must be excluded** to avoid introducing bias.\n",
    "- **Instrumental Variables**: Variables that affect treatment but not outcome (except through treatment). Useful in IV methods but require strong assumptions.\n",
    "\n",
    "**LLM Example**: In analyzing prompt effects:\n",
    "- Include: Task difficulty, prompt length (confounders)\n",
    "- Exclude: Completion tokens (mediator/outcome)\n",
    "- Exclude: User satisfaction rating (collider: affected by both prompt quality and actual quality)\n",
    "\n",
    "### Handling Missing Data in Causal Contexts\n",
    "\n",
    "Missing data is particularly problematic in causal inference:\n",
    "\n",
    "- **Missing Completely at Random (MCAR)**: Missingness independent of all variables. Safe to use simple imputation.\n",
    "- **Missing at Random (MAR)**: Missingness depends on observed data. Use model-based imputation (e.g., MICE).\n",
    "- **Missing Not at Random (MNAR)**: Missingness depends on unobserved data. Requires sensitivity analysis or specialized methods.\n",
    "\n",
    "**Caution**: Don't drop observations randomly! This can violate the positivity assumption and introduce selection bias.\n",
    "\n",
    "### Encoding Categorical Variables\n",
    "\n",
    "- **Binary treatment**: Use 0/1 encoding\n",
    "- **Multi-level treatment**: Use one-hot encoding (n-1 dummy variables)\n",
    "- **Ordinal variables**: Consider whether natural ordering is meaningful for causal interpretation\n",
    "- **Nominal confounders**: One-hot encode, drop one category as reference\n",
    "\n",
    "**Important**: Maintain consistent encoding across treatment and control groups.\n",
    "\n",
    "### Common Support and Overlap\n",
    "\n",
    "The **common support assumption** requires that propensity scores have substantial overlap between treatment groups:\n",
    "\n",
    "```\n",
    "Treatment:     ████████████████████████\n",
    "Control:           ████████████████████████████\n",
    "Overlap:            ████████████████████\n",
    "```\n",
    "\n",
    "Units outside the common support region should be excluded (trimmed) or analyzed separately.\n",
    "\n",
    "### Sensitivity to Preprocessing Choices\n",
    "\n",
    "Causal estimates can be sensitive to:\n",
    "- Scaling of continuous variables\n",
    "- Binning strategies for continuous confounders\n",
    "- Outlier treatment\n",
    "- Feature engineering choices\n",
    "\n",
    "**Best Practice**: Conduct sensitivity analyses across different preprocessing choices to assess robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 LLM-Specific Considerations\n",
    "\n",
    "### Prompts as Interventions/Treatments\n",
    "\n",
    "In LLM research, prompts function as **treatment variables**:\n",
    "\n",
    "- **Binary treatment**: CoT vs. no-CoT\n",
    "- **Continuous treatment**: Temperature, top-k sampling parameters\n",
    "- **Multi-level treatment**: Different prompting strategies (few-shot, zero-shot, chain-of-thought)\n",
    "\n",
    "**Key Challenge**: Prompt effects are often **highly context-dependent**. A prompt that works for mathematical reasoning may not work for creative writing. This motivates careful covariate control (task type, difficulty, domain).\n",
    "\n",
    "### Model Outputs as Outcomes\n",
    "\n",
    "LLM outcomes can be measured in various ways:\n",
    "\n",
    "- **Binary**: Correct/incorrect (classification, QA)\n",
    "- **Ordinal**: Quality score (human evaluation, automated metrics)\n",
    "- **Continuous**: Token likelihood, perplexity\n",
    "- **Multivariate**: Multiple dimensions (accuracy, coherence, creativity)\n",
    "\n",
    "**Consideration**: Different outcome types require different causal estimators. Binary outcomes may use logistic regression; continuous outcomes may use linear models.\n",
    "\n",
    "### Confounders in LLM Experiments\n",
    "\n",
    "Common confounders in LLM research:\n",
    "\n",
    "1. **Task Difficulty**: Harder tasks may receive different prompting strategies\n",
    "2. **Prompt Length**: Longer prompts may correlate with better performance\n",
    "3. **Task Type**: Some prompts work better for certain task categories\n",
    "4. **Token Count**: Longer completions may appear higher quality\n",
    "5. **Domain Knowledge**: Models perform better on topics they've seen more in training\n",
    "\n",
    "**Solution**: Explicitly measure and control for these confounders using domain-specific metrics.\n",
    "\n",
    "### Attention Mechanisms as Causal Mediators\n",
    "\n",
    "Attention weights offer a unique opportunity for **internal causal analysis**:\n",
    "\n",
    "- They provide a mechanistic view of model processing\n",
    "- They can be intervened upon (attention intervention studies)\n",
    "- They vary systematically with prompting strategies\n",
    "- They can be analyzed at different layers and heads\n",
    "\n",
    "**Key Insight**: By analyzing how prompt interventions affect attention patterns, and how those patterns affect final outputs, we can understand *why* prompts work (or don't work).\n",
    "\n",
    "### Reproducibility Challenges\n",
    "\n",
    "LLM experiments face unique reproducibility challenges:\n",
    "\n",
    "1. **Stochasticity**: Random sampling, temperature, seed dependence\n",
    "2. **Hardware**: GPU/CPU differences can affect floating-point computations\n",
    "3. **Versioning**: Model checkpoints, library versions, API changes\n",
    "4. **Evaluation**: Human evaluation variance, automated metric drift\n",
    "\n",
    "**Best Practices**:\n",
    "- Set random seeds (Python, NumPy, PyTorch, CUDA)\n",
    "- Document all versions (model, libraries, CUDA)\n",
    "- Use multiple random seeds and report variance\n",
    "- Provide complete configuration files\n",
    "- Use standardized evaluation benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Dataset Preparation and GPT-2 Testing\n",
    "\n",
    "In this section, we'll:\n",
    "1. Download the two datasets needed for our examples\n",
    "2. Perform exploratory analysis\n",
    "3. Test basic GPT-2 functionality\n",
    "\n",
    "## 2.1 Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face libraries\n",
    "!pip install transformers torch datasets --quiet\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 GPT-2 Model Testing\n",
    "\n",
    "Let's verify that GPT-2 loads and generates text correctly on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "print(\"Loading GPT-2 model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {model.num_parameters:,}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic text generation\n",
    "test_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"To solve this math problem,\",\n",
    "    \"Translate this sentence:\"\n",
    "]\n",
    "\n",
    "print(\"Testing GPT-2 text generation...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=30,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Output: {generated_text}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n✓ GPT-2 is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Example 1 Dataset: Instruction Format Experiment\n",
    "\n",
    "We'll use SuperNaturalInstructions (or a similar instruction-following dataset) to study how different instruction formats affect task completion.\n",
    "\n",
    "If SuperNaturalInstructions is too large or slow, we'll use a smaller subset or an alternative like the FLAN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try loading SuperNaturalInstructions\n",
    "    print(\"Attempting to load SuperNaturalInstructions dataset...\")\n",
    "    dataset1 = load_dataset(\"super_natural_instructions\", split=\"train[:1000]\")\n",
    "    print(f\"✓ Successfully loaded {len(dataset1)} samples from SuperNaturalInstructions\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load SuperNaturalInstructions: {e}\")\n",
    "    print(\"\\nTrying alternative: P3 (Public Pool of Prompts) dataset...\")\n",
    "    try:\n",
    "        dataset1 = load_dataset(\"bigscience/P3\", split=\"train[:1000]\")\n",
    "        print(f\"✓ Successfully loaded {len(dataset1)} samples from P3 dataset\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Could not load P3: {e2}\")\n",
    "        print(\"\\nCreating synthetic instruction format dataset for demonstration...\")\n",
    "        \n",
    "        # Create a synthetic dataset with instruction formats\n",
    "        from datasets import Dataset\n",
    "        \n",
    "        task_types = ['translation', 'qa', 'summarization', 'classification', 'reasoning']\n",
    "        difficulties = ['easy', 'medium', 'hard']\n",
    "        \n",
    "        synthetic_data = []\n",
    "        for i in range(1000):\n",
    "            task = {\n",
    "                'task_id': i,\n",
    "                'task_type': np.random.choice(task_types),\n",
    "                'difficulty': np.random.choice(difficulties),\n",
    "                'instruction': f\"Perform {np.random.choice(task_types)} on this input\",\n",
    "                'input': f\"Sample input text {i}\",\n",
    "            }\n",
    "            synthetic_data.append(task)\n",
    "        \n",
    "        dataset1 = Dataset.from_list(synthetic_data)\n",
    "        print(f\"✓ Created synthetic dataset with {len(dataset1)} samples\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 1 Dataset Sample\")\n",
    "print(\"=\" * 60)\n",
    "print(dataset1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis for Example 1\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df1 = pd.DataFrame(dataset1)\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df1)}\")\n",
    "print(f\"\\nColumns: {list(df1.columns)}\")\n",
    "\n",
    "# Check for categorical variables\n",
    "categorical_cols = df1.select_dtypes(include=['object']).columns\n",
    "print(f\"\\nCategorical variables ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols[:5]:\n",
    "    unique_vals = df1[col].nunique()\n",
    "    print(f\"  - {col}: {unique_vals} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Example 2 Dataset: Attention and Reasoning\n",
    "\n",
    "We'll use GSM8K (Grade School Math) for studying attention patterns in mathematical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K dataset for math reasoning\n",
    "print(\"Loading GSM8K dataset...\")\n",
    "dataset2 = load_dataset(\"gsm8k\", \"main\", split=\"train[:500]\")\n",
    "print(f\"✓ Successfully loaded {len(dataset2)} math problems from GSM8K\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Example 2 Dataset Sample (GSM8K)\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(2):\n",
    "    print(f\"\\n--- Question {i+1} ---\")\n",
    "    print(f\"Question: {dataset2[i]['question'][:100]}...\")\n",
    "    print(f\"Answer: {dataset2[i]['answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis for Example 2\n",
    "df2 = pd.DataFrame(dataset2)\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(df2)}\")\n",
    "print(f\"\\nColumns: {list(df2.columns)}\")\n",
    "\n",
    "# Analyze question and answer lengths\n",
    "df2['question_length'] = df2['question'].str.len()\n",
    "df2['answer_length'] = df2['answer'].str.len()\n",
    "\n",
    "print(f\"\\nQuestion length: mean={df2['question_length'].mean():.1f}, std={df2['question_length'].std():.1f}\")\n",
    "print(f\"Answer length: mean={df2['answer_length'].mean():.1f}, std={df2['answer_length'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Save Processed Datasets\n",
    "\n",
    "We'll save the datasets for use in our example notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to disk\n",
    "output_dir1 = '../Example1_Dataset'\n",
    "output_dir2 = '../Example2_Dataset'\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(output_dir1, exist_ok=True)\n",
    "os.makedirs(output_dir2, exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "df1.to_csv(f'{output_dir1}/instruction_format_data.csv', index=False)\n",
    "df2.to_csv(f'{output_dir2}/attention_reasoning_data.csv', index=False)\n",
    "\n",
    "print(f\"✓ Saved Example 1 dataset to {output_dir1}/instruction_format_data.csv\")\n",
    "print(f\"✓ Saved Example 2 dataset to {output_dir2}/attention_reasoning_data.csv\")\n",
    "print(f\"\\nFile sizes:\")\n",
    "print(f\"  - Example 1: {os.path.getsize(f'{output_dir1}/instruction_format_data.csv') / 1024:.1f} KB\")\n",
    "print(f\"  - Example 2: {os.path.getsize(f'{output_dir2}/attention_reasoning_data.csv') / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Conclusion and Next Steps\n",
    "\n",
    "## Summary of Phase 0\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. ✅ **Established theoretical foundations** in causal inference\n",
    "   - Covered correlation vs causation in LLMs\n",
    "   - Explained the potential outcomes framework\n",
    "   - Introduced causal graphs (DAGs) with LLM examples\n",
    "   - Detailed propensity score matching methodology\n",
    "   - Explored causal mediation analysis\n",
    "   - Discussed data preparation for causal inference\n",
    "   - Addressed LLM-specific considerations\n",
    "\n",
    "2. ✅ **Created visual representations** of causal relationships\n",
    "   - DAG showing confounding in instruction format experiments\n",
    "   - DAG showing mediation in attention mechanisms\n",
    "\n",
    "3. ✅ **Set up computational environment**\n",
    "   - Installed all required libraries\n",
    "   - Tested GPT-2 model functionality\n",
    "\n",
    "4. ✅ **Downloaded and processed datasets**\n",
    "   - Example 1: Instruction format dataset (1000 samples)\n",
    "   - Example 2: GSM8K math reasoning dataset (500 samples)\n",
    "   - Saved processed datasets for efficient reuse\n",
    "\n",
    "## Importance of Data Preparation in Causal Analysis\n",
    "\n",
    "Data preparation is **not** merely a preprocessing step in causal inference—it is fundamental to valid causal claims. Unlike predictive modeling, where feature engineering focuses on maximizing predictive power, causal data preparation must:\n",
    "\n",
    "- **Identify and control for confounders** to avoid biased effect estimates\n",
    "- **Exclude colliders and mediators** (when inappropriate) to prevent spurious associations\n",
    "- **Ensure common support** to make valid within-sample comparisons\n",
    "- **Address missing data** carefully to avoid violating the positivity assumption\n",
    "\n",
    "Poor data preparation can lead to completely incorrect causal conclusions, even with sophisticated estimation methods. This underscores the importance of careful causal diagramming, domain knowledge integration, and rigorous sensitivity analyses.\n",
    "\n",
    "## Impact on Model Development\n",
    "\n",
    "Causal inference transforms model development from \"what works\" to \"why it works\":\n",
    "\n",
    "- **Interpretability**: Understanding causal mechanisms makes models more interpretable and trustworthy\n",
    "- **Optimization**: Causal insights can guide feature engineering and model architecture decisions\n",
    "- **Robustness**: Causal frameworks identify sensitive dependencies and potential failure modes\n",
    "- **Decision-making**: Causal effects support better policy and design decisions in AI systems\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "The foundation is now set for the two main examples:\n",
    "\n",
    "1. **Example 1 Notebook** (`01_example1_psm.ipynb`): Propensity Score Matching for instruction format causality\n",
    "   - Generate instruction format variations\n",
    "   - Measure task completion quality with GPT-2\n",
    "   - Estimate propensity scores\n",
    "   - Perform matching and balance checks\n",
    "   - Estimate and interpret treatment effects\n",
    "\n",
    "2. **Example 2 Notebook** (`02_example2_mediation.ipynb`): Mediation analysis for attention mechanisms\n",
    "   - Generate reasoning tasks with and without CoT\n",
    "   - Extract attention patterns from GPT-2\n",
    "   - Perform mediation analysis\n",
    "   - Visualize indirect effects through attention\n",
    "   - Interpret causal mechanisms\n",
    "\n",
    "## References\n",
    "\n",
    "### Causal Inference Foundations\n",
    "- Pearl, J. (2009). *Causality: Models, Reasoning, and Inference*. Cambridge University Press.\n",
    "- Rubin, D. B. (1974). \"Estimating causal effects of treatments in randomized and nonrandomized studies.\" *Journal of Educational Psychology*, 66(5), 688-701.\n",
    "- Imbens, G. W., & Rubin, D. B. (2015). *Causal Inference for Statistics, Social, and Biomedical Sciences*. Cambridge University Press.\n",
    "\n",
    "### Propensity Score Matching\n",
    "- Rosenbaum, P. R., & Rubin, D. B. (1983). \"The central role of the propensity score in observational studies for causal effects.\" *Biometrika*, 70(1), 41-55.\n",
    "- Stuart, E. A. (2010). \"Matching methods for causal inference: A review and a look forward.\" *Statistical Science*, 25(1), 1-21.\n",
    "\n",
    "### Mediation Analysis\n",
    "- Imai, K., Keele, L., & Tingley, D. (2010). \"A general approach to causal mediation analysis.\" *Psychological Methods*, 15(4), 309-334.\n",
    "- Baron, R. M., & Kenny, D. A. (1986). \"The moderator-mediator variable distinction in social psychological research.\" *Journal of Personality and Social Psychology*, 51(6), 1173-1182.\n",
    "\n",
    "### Causal Inference in Machine Learning\n",
    "- Pearl, J., & Mackenzie, D. (2018). *The Book of Why: The New Science of Cause and Effect*. Basic Books.\n",
    "- Peters, J., Janzing, D., & Schölkopf, B. (2017). *Elements of Causal Inference: Foundations and Learning Algorithms*. MIT Press.\n",
    "\n",
    "### LLM and Attention Mechanisms\n",
    "- Vaswani, A., et al. (2017). \"Attention is all you need.\" *Advances in Neural Information Processing Systems*, 30.\n",
    "- Wei, J., et al. (2022). \"Chain-of-thought prompting elicits reasoning in large language models.\" *Advances in Neural Information Processing Systems*, 35.\n",
    "- Vig, J. (2019). \"A multiscale visualization of attention in the transformer model.\" *ACL 2019 Workshop on BlackboxNLP*.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Phase 0: Setup and Theory**\n",
    "\n",
    "This notebook provides the theoretical foundation and computational setup for the causal analysis examples. The next notebooks will demonstrate practical implementation of these concepts using real LLM data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}